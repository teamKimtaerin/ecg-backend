import logging
from typing import Dict, Any, List, Optional

from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser

from app.core.config import settings
from app.utils.file_utils import response_file_manager
from app.schemas.chatbot import ChatMessage

logger = logging.getLogger(__name__)


class LangChainBedrockService:
    """LangChainì„ ì‚¬ìš©í•œ AWS Bedrock ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        """LangChain ChatBedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.llm = ChatBedrock(
                model_id="us.anthropic.claude-3-5-haiku-20241022-v1:0",
                region_name=settings.aws_bedrock_region,
                credentials_profile_name=None,  # í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
                model_kwargs={
                    "temperature": 0.7,
                    "max_tokens": 1000,
                },
            )

            # ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™”
            self.output_parser = StrOutputParser()

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
            self.system_template = """ë‹¹ì‹ ì€ ECG(Easy Caption Generator) ìë§‰ í¸ì§‘ ë„êµ¬ì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ "ë‘˜ë¦¬"ì…ë‹ˆë‹¤.

ì£¼ìš” ì—­í• :
1. ìë§‰ í¸ì§‘ ê´€ë ¨ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€
2. ECG ë„êµ¬ì˜ ê¸°ëŠ¥ ì‚¬ìš©ë²• ì•ˆë‚´
3. ìë§‰ ì‘ì—… íš¨ìœ¨ì„± ê°œì„  íŒ ì œê³µ
4. ê¸°ìˆ ì  ë¬¸ì œ í•´ê²° ë„ì›€

ë‹µë³€ ìŠ¤íƒ€ì¼:
- ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤
- ê°„ê²°í•˜ë©´ì„œë„ ì¶©ë¶„í•œ ì •ë³´ ì œê³µ
- ë‹¨ê³„ë³„ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš° ëª…í™•í•œ ìˆœì„œë¡œ ì•ˆë‚´
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”

ECG ì£¼ìš” ê¸°ëŠ¥:
- ìë™ ìë§‰ ìƒì„± (AI ìŒì„± ì¸ì‹)
- ì‹¤ì‹œê°„ ìë§‰ í¸ì§‘
- ë‹¤ì–‘í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼
- í™”ì ë¶„ë¦¬ ë° ê´€ë¦¬
- GPU ê°€ì† ë Œë”ë§
- ë“œë˜ê·¸ ì•¤ ë“œë¡­ í¸ì§‘"""

            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„± (ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ í¬í•¨)
            self.prompt_template = ChatPromptTemplate.from_messages(
                [
                    SystemMessagePromptTemplate.from_template(self.system_template),
                    HumanMessagePromptTemplate.from_template(
                        "ì‚¬ìš©ì ìš”ì²­: {input}\n\n"
                        "í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ (ìë§‰ ë° ìŠ¤íƒ€ì¼ë§ ë°ì´í„°):\n"
                        "```json\n{scenario_data}\n```\n\n"
                        "ìœ„ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
                    ),
                ]
            )

            # ì²´ì¸ êµ¬ì„± (í”„ë¡¬í”„íŠ¸ â†’ LLM â†’ íŒŒì„œ)
            self.chain = self.prompt_template | self.llm | self.output_parser

            logger.info(
                f"LangChain ChatBedrock initialized for region: {settings.aws_bedrock_region}"
            )

        except Exception as e:
            logger.error(f"Failed to initialize LangChain ChatBedrock: {e}")
            raise

    def _convert_chat_history_to_messages(
        self, chat_history: List[ChatMessage]
    ) -> List:
        """ChatMessage ë¦¬ìŠ¤íŠ¸ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        messages = []

        # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë§Œ í¬í•¨ (í† í° ì ˆì•½)
        recent_messages = chat_history[-6:] if len(chat_history) > 6 else chat_history

        for msg in recent_messages:
            if msg.sender == "user":
                messages.append(HumanMessage(content=msg.content))
            elif msg.sender == "bot":
                messages.append(AIMessage(content=msg.content))

        return messages

    def invoke_claude_with_chain(
        self,
        prompt: str,
        conversation_history: Optional[List[ChatMessage]] = None,
        scenario_data: Optional[Dict[str, Any]] = None,
        max_tokens: int = 1000,
        temperature: float = 0.7,
        save_response: bool = True,
    ) -> Dict[str, Any]:
        """
        LangChain ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ Claude ëª¨ë¸ í˜¸ì¶œ

        Args:
            prompt: ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ (0.0-1.0)
            save_response: ì‘ë‹µì„ íŒŒì¼ë¡œ ì €ì¥í• ì§€ ì—¬ë¶€

        Returns:
            Dict containing completion and metadata
        """
        try:
            # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
            self.llm.model_kwargs.update(
                {
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                }
            )

            logger.info(
                f"Invoking Claude via LangChain: max_tokens={max_tokens}, temp={temperature}"
            )

            # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì§ì ‘ í¸ì§‘ ì²´ì¸ ì‚¬ìš©
            if scenario_data:
                logger.info(
                    "ğŸ¯ Scenario data detected - using DIRECT SUBTITLE EDIT CHAIN"
                )
                logger.info(
                    f"ğŸ“Š Scenario data size: {len(str(scenario_data))} characters"
                )
                logger.info(f"ğŸ’¬ User prompt: '{prompt}'")

                try:
                    edit_result = self.create_direct_subtitle_edit_chain(
                        user_message=prompt,
                        scenario_data=scenario_data,
                        max_tokens=max_tokens,
                        temperature=temperature,
                    )

                    logger.info("âœ… Direct edit chain completed successfully")
                    logger.info(
                        f"ğŸ“ Edit result type: {edit_result.get('type', 'unknown')}"
                    )
                    logger.info(f"âœ¨ Edit success: {edit_result.get('success', False)}")

                    # í¸ì§‘ ê²°ê³¼ë¥¼ ê¸°ë³¸ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    return {
                        "completion": edit_result.get("explanation", "í¸ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."),
                        "stop_reason": "end_turn",
                        "usage": {
                            "input_tokens": len(prompt.split()),
                            "output_tokens": len(str(edit_result).split()),
                        },
                        "model_id": self.llm.model_id,
                        "langchain_used": True,
                        "edit_result": edit_result,  # ì‹¤ì œ í¸ì§‘ ê²°ê³¼ í¬í•¨
                        "json_patches": edit_result.get("patches", []),  # JSON patch ì •ë³´
                        "request_params": {
                            "max_tokens": max_tokens,
                            "temperature": temperature,
                            "prompt_length": len(prompt),
                            "has_scenario_data": True,
                        },
                    }
                except Exception as e:
                    logger.error(
                        f"âŒ Direct edit chain failed, falling back to standard chain: {e}"
                    )
                    logger.warning(
                        "ğŸ”„ Switching to standard chain processing with scenario context"
                    )
                    # í¸ì§‘ ì²´ì¸ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì²´ì¸ìœ¼ë¡œ fallback

            # ê¸°ë³¸ ì²´ì¸ ì‚¬ìš© (ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì—†ê±°ë‚˜ í¸ì§‘ ì²´ì¸ ì‹¤íŒ¨ì‹œ)
            logger.info("ğŸ”§ Using STANDARD CHAIN processing")

            scenario_json = ""
            if scenario_data:
                try:
                    import json

                    scenario_json = json.dumps(
                        scenario_data, indent=2, ensure_ascii=False
                    )
                    logger.info(
                        f"Scenario data included: {len(scenario_json)} characters"
                    )
                except Exception as e:
                    logger.warning(f"Failed to serialize scenario data: {e}")
                    scenario_json = "ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            else:
                scenario_json = "ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° ì—†ìŒ"

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ê²½ìš° ë©”ëª¨ë¦¬ ì‚¬ìš©
            if conversation_history and len(conversation_history) > 0:
                # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
                memory = ConversationBufferMemory(
                    memory_key="chat_history", return_messages=True
                )

                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€
                messages = self._convert_chat_history_to_messages(conversation_history)
                for message in messages:
                    if isinstance(message, HumanMessage):
                        memory.chat_memory.add_user_message(message.content)
                    elif isinstance(message, AIMessage):
                        memory.chat_memory.add_ai_message(message.content)

                # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° í¬í•¨)
                history_prompt = ChatPromptTemplate.from_messages(
                    [
                        SystemMessagePromptTemplate.from_template(self.system_template),
                        *messages,
                        HumanMessagePromptTemplate.from_template(
                            "ì‚¬ìš©ì ìš”ì²­: {input}\n\n"
                            "í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ (ìë§‰ ë° ìŠ¤íƒ€ì¼ë§ ë°ì´í„°):\n"
                            "```json\n{scenario_data}\n```\n\n"
                            "ìœ„ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
                        ),
                    ]
                )

                # íˆìŠ¤í† ë¦¬ í¬í•¨ ì²´ì¸
                history_chain = history_prompt | self.llm | self.output_parser
                completion = history_chain.invoke(
                    {"input": prompt, "scenario_data": scenario_json}
                )
            else:
                # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ì™€ í•¨ê»˜ ë‹¨ìˆœ ì²´ì¸ ì‚¬ìš©
                completion = self.chain.invoke(
                    {"input": prompt, "scenario_data": scenario_json}
                )

            logger.info("LangChain Claude invocation successful")
            logger.debug(f"Response preview: {completion[:200]}...")

            # ì‘ë‹µ ê²°ê³¼ êµ¬ì„±
            result = {
                "completion": completion,
                "stop_reason": "end_turn",  # LangChainì—ì„œëŠ” ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’
                "usage": {
                    "input_tokens": len(prompt.split()),  # ê·¼ì‚¬ì¹˜
                    "output_tokens": len(completion.split()),  # ê·¼ì‚¬ì¹˜
                },
                "model_id": self.llm.model_id,
                "langchain_used": True,
                "request_params": {
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "prompt_length": len(prompt),
                    "history_length": len(conversation_history)
                    if conversation_history
                    else 0,
                },
            }

            # ì‘ë‹µ ì €ì¥ (ì˜µì…˜)
            if save_response:
                try:
                    # JSON í˜•íƒœë¡œ ì „ì²´ ì‘ë‹µ ì €ì¥
                    json_file_path = response_file_manager.save_response(
                        data=result, prefix="langchain_bedrock_response"
                    )

                    # í…ìŠ¤íŠ¸ë§Œ ë³„ë„ ì €ì¥
                    text_file_path = response_file_manager.save_text_response(
                        text=completion,
                        prefix="langchain_bedrock_completion",
                        metadata={
                            "model_id": self.llm.model_id,
                            "langchain_used": True,
                            "temperature": temperature,
                            "max_tokens": max_tokens,
                        },
                    )

                    result["saved_files"] = {
                        "json_file": json_file_path,
                        "text_file": text_file_path,
                    }

                    logger.info(
                        f"LangChain response saved to files: {json_file_path}, {text_file_path}"
                    )

                except Exception as save_error:
                    logger.warning(f"Failed to save LangChain response: {save_error}")
                    result["save_error"] = str(save_error)

            return result

        except Exception as e:
            logger.error(f"LangChain Claude invocation failed: {e}")

            # ì—ëŸ¬ íƒ€ì…ë³„ ì²˜ë¦¬
            if "credentials" in str(e).lower() or "access" in str(e).lower():
                raise Exception("AWS ìê²©ì¦ëª…ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            elif "throttling" in str(e).lower() or "rate" in str(e).lower():
                raise Exception("API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            elif "validation" in str(e).lower():
                raise Exception("ìš”ì²­ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            else:
                raise Exception(f"LangChainì„ í†µí•œ Claude í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

    def test_connection(self) -> bool:
        """
        LangChainì„ í†µí•œ Bedrock ì—°ê²° í…ŒìŠ¤íŠ¸

        Returns:
            bool: ì—°ê²° ì„±ê³µ ì—¬ë¶€
        """
        try:
            test_result = self.invoke_claude_with_chain(
                prompt="ì•ˆë…•í•˜ì„¸ìš”",
                max_tokens=50,
                temperature=0.1,
                save_response=False,
            )
            return "completion" in test_result and len(test_result["completion"]) > 0
        except Exception as e:
            logger.error(f"LangChain connection test failed: {e}")
            return False

    def get_saved_responses(self, pattern: str = "langchain_bedrock_*") -> List[str]:
        """
        ì €ì¥ëœ LangChain ì‘ë‹µ íŒŒì¼ ëª©ë¡ ì¡°íšŒ

        Args:
            pattern: íŒŒì¼ íŒ¨í„´

        Returns:
            List[str]: íŒŒì¼ ëª©ë¡
        """
        return response_file_manager.list_saved_files(pattern)

    def get_response_file_info(self, filename: str) -> Dict[str, Any]:
        """
        LangChain ì‘ë‹µ íŒŒì¼ ì •ë³´ ì¡°íšŒ

        Args:
            filename: íŒŒì¼ëª…

        Returns:
            Dict: íŒŒì¼ ì •ë³´
        """
        return response_file_manager.get_file_info(filename)

    def create_multi_step_chain(
        self,
        steps: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ì²´ì¸ ì‹¤í–‰

        Args:
            steps: ë‹¨ê³„ë³„ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ [{"step": "1", "prompt": "..."}, ...]
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ

        Returns:
            Dict: ê° ë‹¨ê³„ë³„ ê²°ê³¼ì™€ ìµœì¢… ê²°ê³¼
        """
        try:
            results = {}
            accumulated_context = ""

            logger.info(f"Starting multi-step chain with {len(steps)} steps")

            for i, step_info in enumerate(steps, 1):
                step_name = step_info.get("step", f"step_{i}")
                step_prompt = step_info.get("prompt", "")

                # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
                if accumulated_context:
                    full_prompt = (
                        f"ì´ì „ ë‹¨ê³„ ê²°ê³¼:\n{accumulated_context}\n\ní˜„ì¬ ë‹¨ê³„: {step_prompt}"
                    )
                else:
                    full_prompt = step_prompt

                logger.info(f"Executing step {i}/{len(steps)}: {step_name}")

                # ê° ë‹¨ê³„ë³„ ì²´ì¸ ì‹¤í–‰
                step_result = self.invoke_claude_with_chain(
                    prompt=full_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    save_response=False,
                )

                results[step_name] = {
                    "prompt": step_prompt,
                    "response": step_result["completion"],
                    "step_number": i,
                    "usage": step_result.get("usage", {}),
                }

                # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ ê²°ê³¼ ëˆ„ì 
                accumulated_context += f"\n[{step_name}] {step_result['completion']}"

            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_result = {
                "multi_step_results": results,
                "final_context": accumulated_context,
                "total_steps": len(steps),
                "langchain_used": True,
                "chain_type": "multi_step",
            }

            logger.info(
                f"Multi-step chain completed successfully with {len(steps)} steps"
            )
            return final_result

        except Exception as e:
            logger.error(f"Multi-step chain failed: {e}")
            raise Exception(f"ë‹¤ë‹¨ê³„ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def create_parallel_chain(
        self,
        parallel_prompts: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ì²´ì¸

        Args:
            parallel_prompts: ë³‘ë ¬ ì‹¤í–‰í•  í”„ë¡¬í”„íŠ¸ë“¤ [{"name": "task1", "prompt": "..."}, ...]
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ

        Returns:
            Dict: ê° ë³‘ë ¬ ì‘ì—…ë³„ ê²°ê³¼
        """
        try:
            results = {}

            logger.info(f"Starting parallel chain with {len(parallel_prompts)} tasks")

            for task_info in parallel_prompts:
                task_name = task_info.get("name", f"task_{len(results) + 1}")
                task_prompt = task_info.get("prompt", "")

                logger.info(f"Executing parallel task: {task_name}")

                # ê° ì‘ì—…ë³„ ë…ë¦½ì ì¸ ì²´ì¸ ì‹¤í–‰
                task_result = self.invoke_claude_with_chain(
                    prompt=task_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    save_response=False,
                )

                results[task_name] = {
                    "prompt": task_prompt,
                    "response": task_result["completion"],
                    "usage": task_result.get("usage", {}),
                    "model_id": task_result.get("model_id"),
                }

            # ìµœì¢… ê²°ê³¼ êµ¬ì„±
            final_result = {
                "parallel_results": results,
                "total_tasks": len(parallel_prompts),
                "langchain_used": True,
                "chain_type": "parallel",
            }

            logger.info(
                f"Parallel chain completed successfully with {len(parallel_prompts)} tasks"
            )
            return final_result

        except Exception as e:
            logger.error(f"Parallel chain failed: {e}")
            raise Exception(f"ë³‘ë ¬ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def create_conditional_chain(
        self,
        initial_prompt: str,
        conditions: List[Dict[str, Any]],
        max_tokens: int = 1000,
        temperature: float = 0.7,
    ) -> Dict[str, Any]:
        """
        ì¡°ê±´ë¶€ ì²´ì¸ ì‹¤í–‰ (ì²« ë²ˆì§¸ ì‘ë‹µì— ë”°ë¼ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •)

        Args:
            initial_prompt: ì´ˆê¸° í”„ë¡¬í”„íŠ¸
            conditions: ì¡°ê±´ë³„ ë‹¤ìŒ ë‹¨ê³„ë“¤ [{"condition_keyword": "í‚¤ì›Œë“œ", "next_prompt": "..."}, ...]
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ

        Returns:
            Dict: ì¡°ê±´ë¶€ ì‹¤í–‰ ê²°ê³¼
        """
        try:
            logger.info("Starting conditional chain")

            # 1ë‹¨ê³„: ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
            initial_result = self.invoke_claude_with_chain(
                prompt=initial_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                save_response=False,
            )

            initial_response = initial_result["completion"].lower()

            # 2ë‹¨ê³„: ì¡°ê±´ í™•ì¸ ë° ë‹¤ìŒ ë‹¨ê³„ ê²°ì •
            matched_condition = None
            for condition in conditions:
                condition_keyword = condition.get("condition_keyword", "").lower()
                if condition_keyword and condition_keyword in initial_response:
                    matched_condition = condition
                    break

            if matched_condition:
                logger.info(
                    f"Condition matched: {matched_condition.get('condition_keyword')}"
                )

                # ì¡°ê±´ì— ë§ëŠ” ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰
                next_prompt = matched_condition.get("next_prompt", "")
                context_prompt = (
                    f"ì´ì „ ì‘ë‹µ: {initial_result['completion']}\n\n{next_prompt}"
                )

                next_result = self.invoke_claude_with_chain(
                    prompt=context_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    save_response=False,
                )

                final_result = {
                    "initial_step": {
                        "prompt": initial_prompt,
                        "response": initial_result["completion"],
                        "usage": initial_result.get("usage", {}),
                    },
                    "conditional_step": {
                        "matched_condition": matched_condition.get("condition_keyword"),
                        "prompt": next_prompt,
                        "response": next_result["completion"],
                        "usage": next_result.get("usage", {}),
                    },
                    "final_response": next_result["completion"],
                    "langchain_used": True,
                    "chain_type": "conditional",
                }
            else:
                logger.info("No condition matched, using initial response")
                final_result = {
                    "initial_step": {
                        "prompt": initial_prompt,
                        "response": initial_result["completion"],
                        "usage": initial_result.get("usage", {}),
                    },
                    "conditional_step": None,
                    "final_response": initial_result["completion"],
                    "langchain_used": True,
                    "chain_type": "conditional",
                }

            logger.info("Conditional chain completed successfully")
            return final_result

        except Exception as e:
            logger.error(f"Conditional chain failed: {e}")
            raise Exception(f"ì¡°ê±´ë¶€ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def create_subtitle_animation_chain(
        self,
        user_message: str,
        subtitle_json: Optional[Dict[str, Any]] = None,
        max_tokens: int = 1500,
        temperature: float = 0.3,
    ) -> Dict[str, Any]:
        """
        ECG ìë§‰ ì• ë‹ˆë©”ì´ì…˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŠ¹í™”ëœ ìˆœì°¨ ì²´ì¸
        Prompt.mdì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ë”°ë¦„

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            subtitle_json: ê¸°ì¡´ ìë§‰ JSON ë°ì´í„° (ì„ íƒì‚¬í•­)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ (ë‚®ì€ ê°’ìœ¼ë¡œ ì •í™•ì„± ìš°ì„ )

        Returns:
            Dict: ë‹¨ê³„ë³„ ì²˜ë¦¬ ê²°ê³¼ì™€ ìµœì¢… JSON patch
        """
        try:
            logger.info("Starting ECG subtitle animation chain")

            # ë‹¨ê³„ 1: ë©”ì‹œì§€ ìœ í˜• ë¶„ë¥˜
            classification_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ìœ í˜•ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"

ë¶„ë¥˜ ê¸°ì¤€:
1. "simple_info" - ë‹¨ìˆœí•œ ì •ë³´ ìš”ì²­
2. "simple_edit" - ê°„ë‹¨í•œ ìë§‰ ìˆ˜ì • (ì˜¤íƒˆì, ë²ˆì—­ ë“±)
3. "animation_request" - ìë§‰ ì• ë‹ˆë©”ì´ì…˜ ìˆ˜ì •/ì¶”ê°€ ìš”ì²­

ì‘ë‹µ í˜•ì‹:
{{
    "classification": "ë¶„ë¥˜ê²°ê³¼",
    "confidence": 0.95,
    "reasoning": "ë¶„ë¥˜ ê·¼ê±°"
}}"""

            step1_result = self.invoke_claude_with_chain(
                prompt=classification_prompt,
                max_tokens=200,
                temperature=0.1,
                save_response=False,
            )

            logger.info("Step 1 completed: Message classification")

            # ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹± ì‹œë„
            logger.info(
                f"ğŸ” Raw AI classification response: {step1_result['completion']}"
            )

            try:
                import json

                classification_data = json.loads(step1_result["completion"])
                classification = classification_data.get(
                    "classification", "animation_request"
                )
                confidence = classification_data.get("confidence", "unknown")
                reasoning = classification_data.get(
                    "reasoning", "No reasoning provided"
                )

                logger.info(
                    f"âœ… JSON parsing successful - Classification: {classification}, Confidence: {confidence}"
                )
                logger.info(f"ğŸ“ AI reasoning: {reasoning}")

            except (json.JSONDecodeError, KeyError, TypeError) as e:
                logger.warning(
                    f"âŒ JSON parsing failed ({type(e).__name__}: {e}), falling back to keyword-based classification"
                )

                # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
                response_lower = step1_result["completion"].lower()
                logger.info(
                    f"ğŸ”¤ Analyzing keywords in lowercase response: '{response_lower[:200]}...'"
                )

                if "simple_info" in response_lower or "ì •ë³´" in response_lower:
                    classification = "simple_info"
                    logger.info("ğŸ¯ Keyword match: 'simple_info' or 'ì •ë³´' found")
                elif "simple_edit" in response_lower or "ê°„ë‹¨" in response_lower:
                    classification = "simple_edit"
                    logger.info("ğŸ¯ Keyword match: 'simple_edit' or 'ê°„ë‹¨' found")
                else:
                    classification = "animation_request"
                    logger.info("ğŸ¯ Default fallback: classified as 'animation_request'")

            logger.info(
                f"ğŸ·ï¸  FINAL CLASSIFICATION: '{classification}' for user message: '{user_message}')"
            )

            # ë‹¨ê³„ 2: ë¶„ë¥˜ì— ë”°ë¥¸ ì²˜ë¦¬
            logger.info(f"ğŸ”„ Processing classification: {classification}")

            if classification == "simple_info":
                logger.info(
                    "ğŸ“š Processing as SIMPLE_INFO request - providing general information"
                )
                # ë‹¨ìˆœ ì •ë³´ ìš”ì²­ - ë°”ë¡œ ì‘ë‹µ
                info_prompt = f"""ECG ìë§‰ í¸ì§‘ ë„êµ¬ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {user_message}

ECG ì£¼ìš” ê¸°ëŠ¥:
- ìë™ ìë§‰ ìƒì„± (AI ìŒì„± ì¸ì‹)
- ì‹¤ì‹œê°„ ìë§‰ í¸ì§‘
- ë‹¤ì–‘í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼
- í™”ì ë¶„ë¦¬ ë° ê´€ë¦¬
- GPU ê°€ì† ë Œë”ë§
- ë“œë˜ê·¸ ì•¤ ë“œë¡­ í¸ì§‘

ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

                final_result = self.invoke_claude_with_chain(
                    prompt=info_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    save_response=False,
                )

                return {
                    "chain_type": "subtitle_animation",
                    "classification": "simple_info",
                    "steps": {
                        "classification": step1_result,
                        "final_response": final_result,
                    },
                    "final_response": final_result["completion"],
                    "json_patch": None,
                    "langchain_used": True,
                }

            elif classification == "simple_edit":
                logger.info(
                    "âœï¸  Processing as SIMPLE_EDIT request - modifying subtitle text/style"
                )
                # ê°„ë‹¨í•œ ìë§‰ ìˆ˜ì •
                if not subtitle_json:
                    return {
                        "chain_type": "subtitle_animation",
                        "classification": "simple_edit",
                        "error": "ìë§‰ JSON ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                        "langchain_used": True,
                    }

                edit_prompt = f"""ê¸°ì¡´ ìë§‰ JSONì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”ì²­ì‚¬í•­ì„ JSON patch í˜•íƒœë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: {user_message}
ê¸°ì¡´ ìë§‰ JSON: {json.dumps(subtitle_json, ensure_ascii=False, indent=2)}

JSON patch í˜•íƒœë¡œ ìˆ˜ì •ì‚¬í•­ì„ ì œê³µí•´ì£¼ì„¸ìš”. ê¸°ì¡´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ìˆ˜ì •í•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{{
    "patches": [
        {{"op": "replace", "path": "/clips/0/text", "value": "ìˆ˜ì •ëœ í…ìŠ¤íŠ¸"}},
        {{"op": "add", "path": "/clips/1/style/color", "value": "#FF0000"}}
    ],
    "summary": "ìˆ˜ì • ë‚´ìš© ìš”ì•½"
}}"""

                edit_result = self.invoke_claude_with_chain(
                    prompt=edit_prompt,
                    max_tokens=max_tokens,
                    temperature=0.1,
                    save_response=False,
                )

                return {
                    "chain_type": "subtitle_animation",
                    "classification": "simple_edit",
                    "steps": {
                        "classification": step1_result,
                        "edit_result": edit_result,
                    },
                    "final_response": edit_result["completion"],
                    "json_patch": edit_result["completion"],
                    "langchain_used": True,
                }

            else:  # animation_request
                logger.info(
                    "ğŸ¬ Processing as ANIMATION_REQUEST - extracting animation category and generating effects"
                )
                # ë‹¨ê³„ 3: ì• ë‹ˆë©”ì´ì…˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ì‹¤ì œ manifest ê¸°ë°˜)
                category_prompt = f"""ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"

ì‚¬ìš© ê°€ëŠ¥í•œ ì• ë‹ˆë©”ì´ì…˜ ì¹´í…Œê³ ë¦¬ (ì‹¤ì œ manifest ê¸°ë°˜):

1. **rotation** - 3D íšŒì „ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: rotationDegrees(90-720Â°), animationDuration(0.5-3s), axisX/Y/Z(boolean), perspective(200-1500px), staggerDelay(0-0.3s)

2. **fadein** - í˜ì´ë“œ ì¸ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: staggerDelay(0.02-0.5s), animationDuration(0.2-2s), startOpacity(0-0.5), scaleStart(0.5-1), ease(power1-3.out/back.out/elastic.out)

3. **typewriter** - íƒ€ì´í•‘ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: typingSpeed(0.02-0.2s), cursorBlink(boolean), cursorChar(string), showCursor(boolean), soundEffect(boolean)

4. **glow** - ê¸€ë¡œìš° íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: color(#hex), intensity(0-1), pulse(boolean), cycles(1-120)

5. **scalepop** - ìŠ¤ì¼€ì¼ íŒ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: popScale(1.1-3), animationDuration(0.5-2.5s), staggerDelay(0-0.3s), bounceStrength(0.1-2), colorPop(boolean)

6. **slideup** - ìŠ¬ë¼ì´ë“œ ì—… íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: slideDistance(10-100px), animationDuration(0.3-2s), staggerDelay(0-0.5s), easeType(power2.out/back.out/elastic.out/bounce.out), blurEffect(boolean)

7. **elastic** - íƒ„ì„± íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: bounceStrength(0.1-2), animationDuration(0.5-4s), staggerDelay(0-0.5s), startScale(0-1), overshoot(1-2)

8. **glitch** - ê¸€ë¦¬ì¹˜ íš¨ê³¼
   - íŒŒë¼ë¯¸í„°: glitchIntensity(1-20px), animationDuration(0.5-5s), glitchFrequency(0.1-1s), colorSeparation(boolean), noiseEffect(boolean)

9. **flames** - ë¶ˆê½ƒ íš¨ê³¼ (GIF ê¸°ë°˜)
   - íŒŒë¼ë¯¸í„°: baseOpacity(0-1), flicker(0-1), cycles(1-120)

10. **pulse** - í„ìŠ¤ íš¨ê³¼
    - íŒŒë¼ë¯¸í„°: maxScale(1-2.5), cycles(0-10)

ë§Œì•½ ì¹´í…Œê³ ë¦¬ê°€ ëª…í™•í•˜ì§€ ì•Šë‹¤ë©´ ì‚¬ìš©ìì—ê²Œ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì œê³µí•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{{
    "category": "ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬ëª…",
    "confidence": 0.85,
    "suggested_parameters": {{
        "animationDuration": 1.5,
        "staggerDelay": 0.1
    }},
    "clarification_needed": false,
    "user_intent": "ì‚¬ìš©ì ì˜ë„ ë¶„ì„"
}}"""

                step3_result = self.invoke_claude_with_chain(
                    prompt=category_prompt,
                    max_tokens=300,
                    temperature=0.2,
                    save_response=False,
                )

                # ë‹¨ê³„ 4: ì• ë‹ˆë©”ì´ì…˜ JSON ìƒì„± (manifest ìŠ¤í‚¤ë§ˆ ê¸°ë°˜)
                animation_prompt = f"""ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ manifest ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ì• ë‹ˆë©”ì´ì…˜ JSONì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì¹´í…Œê³ ë¦¬ ë¶„ì„ ê²°ê³¼: {step3_result["completion"]}
ì‚¬ìš©ì ì›ë³¸ ìš”ì²­: {user_message}

ì‹¤ì œ manifest ìŠ¤í‚¤ë§ˆì— ë”°ë¼ JSON patchë¥¼ ìƒì„±í•˜ì„¸ìš”:

ì˜ˆì‹œ êµ¬ì¡°:
- **rotation**: {{"rotationDegrees": 360, "animationDuration": 1.5, "axisY": true, "perspective": 800, "staggerDelay": 0.1}}
- **fadein**: {{"staggerDelay": 0.1, "animationDuration": 0.8, "startOpacity": 0, "scaleStart": 0.9, "ease": "power2.out"}}
- **typewriter**: {{"typingSpeed": 0.05, "cursorBlink": true, "cursorChar": "|", "showCursor": true}}
- **glow**: {{"color": "#00ffff", "intensity": 0.4, "pulse": true, "cycles": 8}}
- **scalepop**: {{"popScale": 1.5, "animationDuration": 1.2, "staggerDelay": 0.08, "bounceStrength": 0.6, "colorPop": true}}
- **slideup**: {{"slideDistance": 30, "animationDuration": 1, "staggerDelay": 0.12, "easeType": "power2.out", "blurEffect": true}}
- **elastic**: {{"bounceStrength": 0.7, "animationDuration": 1.5, "staggerDelay": 0.1, "startScale": 0, "overshoot": 1.3}}
- **glitch**: {{"glitchIntensity": 5, "animationDuration": 2, "glitchFrequency": 0.3, "colorSeparation": true, "noiseEffect": true}}
- **flames**: {{"baseOpacity": 0.8, "flicker": 0.3, "cycles": 12}}
- **pulse**: {{"maxScale": 1.2, "cycles": 1}}

ì‘ë‹µ í˜•ì‹ (JSON patch):
{{
    "patches": [
        {{
            "op": "add",
            "path": "/clips/0/animation",
            "value": {{
                "plugin": "ì¹´í…Œê³ ë¦¬ëª…@2.0.0",
                "manifest": {{
                    "name": "ì¹´í…Œê³ ë¦¬ëª…",
                    "version": "2.0.0"
                }},
                "parameters": {{
                    "ì‹¤ì œìŠ¤í‚¤ë§ˆíŒŒë¼ë¯¸í„°ë“¤": "ì ì ˆí•œê°’"
                }}
            }}
        }}
    ],
    "animation_summary": "ì ìš©ëœ ì• ë‹ˆë©”ì´ì…˜ì˜ ìƒì„¸ ì„¤ëª…",
    "manifest_used": {{
        "plugin": "ì¹´í…Œê³ ë¦¬ëª…@2.0.0",
        "parameters_count": 5,
        "schema_validated": true
    }}
}}"""

                final_result = self.invoke_claude_with_chain(
                    prompt=animation_prompt,
                    max_tokens=max_tokens,
                    temperature=0.3,
                    save_response=False,
                )

                return {
                    "chain_type": "subtitle_animation",
                    "classification": "animation_request",
                    "steps": {
                        "classification": step1_result,
                        "category_extraction": step3_result,
                        "animation_generation": final_result,
                    },
                    "final_response": final_result["completion"],
                    "json_patch": final_result["completion"],
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Subtitle animation chain failed: {e}")
            raise Exception(f"ìë§‰ ì• ë‹ˆë©”ì´ì…˜ ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")

    def create_direct_subtitle_edit_chain(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int = 1500,
        temperature: float = 0.1,
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ëŠ” ì²´ì¸

        Args:
            user_message: ì‚¬ìš©ì í¸ì§‘ ìš”ì²­
            scenario_data: í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤ JSON ë°ì´í„°
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± ì¡°ì ˆ (ì •í™•ì„±ì„ ìœ„í•´ ë‚®ê²Œ ì„¤ì •)

        Returns:
            Dict: JSON patchì™€ í¸ì§‘ ê²°ê³¼
        """
        try:
            logger.info("Starting direct subtitle edit chain")

            # 1ë‹¨ê³„: ìš”ì²­ ìœ í˜• ë¶„ë¥˜
            classification_prompt = f"""ì‚¬ìš©ìì˜ ìë§‰ í¸ì§‘ ìš”ì²­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ìš”ì²­: "{user_message}"

ë¶„ë¥˜ ê¸°ì¤€:
- "text_edit": ìë§‰ í…ìŠ¤íŠ¸ ìˆ˜ì • (ì˜¤íƒˆì, ë²ˆì—­, ë‹¨ì–´ ë³€ê²½)
- "style_edit": ìë§‰ ìŠ¤íƒ€ì¼ ìˆ˜ì • (ìƒ‰ìƒ, í¬ê¸°, ìœ„ì¹˜)
- "animation_request": ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ ì¶”ê°€/ìˆ˜ì •
- "info_request": ë‹¨ìˆœ ì •ë³´ ì§ˆë¬¸

JSON í˜•íƒœë¡œ ì‘ë‹µ:
{{"classification": "ë¶„ë¥˜ê²°ê³¼", "confidence": 0.95}}"""

            classification_result = self.invoke_claude_with_chain(
                prompt=classification_prompt,
                max_tokens=200,
                temperature=0.1,
                save_response=False,
            )

            logger.info(
                f"ğŸ” [DIRECT EDIT] Raw AI classification response: {classification_result['completion']}"
            )

            # ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹±
            try:
                import json

                classification_data = json.loads(classification_result["completion"])
                classification = classification_data.get("classification", "text_edit")
                confidence = classification_data.get("confidence", "unknown")

                logger.info(
                    f"âœ… [DIRECT EDIT] JSON parsing successful - Classification: {classification}, Confidence: {confidence}"
                )

            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(
                    f"âŒ [DIRECT EDIT] JSON parsing failed ({type(e).__name__}: {e}), using fallback classification"
                )
                classification = "text_edit"  # ê¸°ë³¸ê°’

            logger.info(
                f"ğŸ·ï¸ [DIRECT EDIT] FINAL CLASSIFICATION: '{classification}' for user message: '{user_message}'"
            )

            # 2ë‹¨ê³„: ë¶„ë¥˜ì— ë”°ë¥¸ í¸ì§‘ ì‹¤í–‰
            logger.info(f"ğŸ”„ [DIRECT EDIT] Dispatching to handler for: {classification}")

            if classification == "text_edit":
                logger.info("ğŸ“ [DIRECT EDIT] Calling text edit handler")
                return self._handle_text_edit(
                    user_message, scenario_data, max_tokens, temperature
                )
            elif classification == "style_edit":
                logger.info("ğŸ¨ [DIRECT EDIT] Calling style edit handler")
                return self._handle_style_edit(
                    user_message, scenario_data, max_tokens, temperature
                )
            elif classification == "animation_request":
                logger.info("ğŸ¬ [DIRECT EDIT] Calling animation request handler")
                return self._handle_animation_request(
                    user_message, scenario_data, max_tokens, temperature
                )
            else:
                logger.info("ğŸ“š [DIRECT EDIT] Calling info request handler")
                return self._handle_info_request(user_message, max_tokens, temperature)

        except Exception as e:
            logger.error(f"Direct subtitle edit chain failed: {e}")
            return {
                "type": "error",
                "error": f"í¸ì§‘ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_text_edit(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int,
        temperature: float,
    ) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìˆ˜ì • ì²˜ë¦¬"""
        try:
            import json

            scenario_json = json.dumps(scenario_data, indent=2, ensure_ascii=False)

            edit_prompt = f"""ECG ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìë§‰ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_message}"

í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤:
```json
{scenario_json}
```

ì‘ì—…:
1. ìˆ˜ì •í•  ìë§‰ì„ ì°¾ê¸° (ì²« ë²ˆì§¸, ë‘ ë²ˆì§¸, íŠ¹ì • ë‹¨ì–´ ë“±)
2. ìš”ì²­ì— ë§ê²Œ í…ìŠ¤íŠ¸ ìˆ˜ì •
3. JSON patch ìƒì„±

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
    "type": "text_edit",
    "patches": [
        {{"op": "replace", "path": "/cues/0/root/children/0/text", "value": "ìˆ˜ì •ëœ í…ìŠ¤íŠ¸"}}
    ],
    "explanation": "ìˆ˜ì • ì„¤ëª…",
    "success": true
}}"""

            result = self.invoke_claude_with_chain(
                prompt=edit_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                save_response=False,
            )

            # JSON ì‘ë‹µ íŒŒì‹± ì‹œë„
            try:
                import json

                edit_result = json.loads(result["completion"])
                edit_result["langchain_used"] = True
                return edit_result
            except json.JSONDecodeError:
                return {
                    "type": "text_edit",
                    "patches": [],
                    "explanation": result["completion"],
                    "success": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Text edit handling failed: {e}")
            return {
                "type": "text_edit",
                "error": f"í…ìŠ¤íŠ¸ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_style_edit(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int,
        temperature: float,
    ) -> Dict[str, Any]:
        """ìŠ¤íƒ€ì¼ ìˆ˜ì • ì²˜ë¦¬"""
        try:
            import json

            scenario_json = json.dumps(scenario_data, indent=2, ensure_ascii=False)

            style_prompt = f"""ECG ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìë§‰ ìŠ¤íƒ€ì¼ì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_message}"

í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤:
```json
{scenario_json}
```

ìŠ¤íƒ€ì¼ ì†ì„±:
- color: ìƒ‰ìƒ (ì˜ˆ: "#ff0000", "#00ff00")
- fontSize: í¬ê¸° (ì˜ˆ: 24, 32)
- fontWeight: êµµê¸° (ì˜ˆ: "bold", "normal")
- textAlign: ì •ë ¬ (ì˜ˆ: "center", "left")

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
    "type": "style_edit",
    "patches": [
        {{"op": "replace", "path": "/cues/0/root/children/0/style/color", "value": "#ff0000"}}
    ],
    "explanation": "ìŠ¤íƒ€ì¼ ìˆ˜ì • ì„¤ëª…",
    "success": true
}}"""

            result = self.invoke_claude_with_chain(
                prompt=style_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                save_response=False,
            )

            try:
                import json

                edit_result = json.loads(result["completion"])
                edit_result["langchain_used"] = True
                return edit_result
            except json.JSONDecodeError:
                return {
                    "type": "style_edit",
                    "patches": [],
                    "explanation": result["completion"],
                    "success": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Style edit handling failed: {e}")
            return {
                "type": "style_edit",
                "error": f"ìŠ¤íƒ€ì¼ ìˆ˜ì • ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_animation_request(
        self,
        user_message: str,
        scenario_data: Dict[str, Any],
        max_tokens: int,
        temperature: float,
    ) -> Dict[str, Any]:
        """ì• ë‹ˆë©”ì´ì…˜ ìš”ì²­ ì²˜ë¦¬"""
        try:
            import json

            scenario_json = json.dumps(scenario_data, indent=2, ensure_ascii=False)

            animation_prompt = f"""ECG ì‹œë‚˜ë¦¬ì˜¤ì— ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_message}"

í˜„ì¬ ì‹œë‚˜ë¦¬ì˜¤:
```json
{scenario_json}
```

ì‚¬ìš© ê°€ëŠ¥í•œ ì• ë‹ˆë©”ì´ì…˜:
- **rotation**: íšŒì „ íš¨ê³¼ {{"rotationDegrees": 360, "animationDuration": 1.0}}
- **fadein**: í˜ì´ë“œì¸ {{"animationDuration": 1.0, "startOpacity": 0}}
- **typewriter**: íƒ€ì´í•‘ {{"typingSpeed": 50, "showCursor": true}}
- **glow**: ê¸€ë¡œìš° {{"color": "#ffff00", "intensity": 2, "pulse": true}}
- **scalepop**: íŒ íš¨ê³¼ {{"popScale": 1.5, "animationDuration": 1.2}}

ì‘ë‹µ í˜•ì‹ (JSONë§Œ):
{{
    "type": "animation_request",
    "patches": [
        {{"op": "add", "path": "/cues/0/root/children/0/pluginChain", "value": [{{"name": "fadein", "params": {{"animationDuration": 1.0}}}}]}}
    ],
    "explanation": "ì• ë‹ˆë©”ì´ì…˜ ì¶”ê°€ ì„¤ëª…",
    "success": true
}}"""

            result = self.invoke_claude_with_chain(
                prompt=animation_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                save_response=False,
            )

            try:
                import json

                edit_result = json.loads(result["completion"])
                edit_result["langchain_used"] = True
                return edit_result
            except json.JSONDecodeError:
                return {
                    "type": "animation_request",
                    "patches": [],
                    "explanation": result["completion"],
                    "success": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "langchain_used": True,
                }

        except Exception as e:
            logger.error(f"Animation request handling failed: {e}")
            return {
                "type": "animation_request",
                "error": f"ì• ë‹ˆë©”ì´ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }

    def _handle_info_request(
        self, user_message: str, max_tokens: int, temperature: float
    ) -> Dict[str, Any]:
        """ì •ë³´ ìš”ì²­ ì²˜ë¦¬"""
        try:
            info_prompt = f"""ECG ìë§‰ í¸ì§‘ ë„êµ¬ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{user_message}"

ECG ì£¼ìš” ê¸°ëŠ¥:
- ìë™ ìë§‰ ìƒì„± ë° í¸ì§‘
- ë‹¤ì–‘í•œ ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼
- ì‹¤ì‹œê°„ ë¯¸ë¦¬ë³´ê¸°
- GPU ê°€ì† ë Œë”ë§

ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

            result = self.invoke_claude_with_chain(
                prompt=info_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                save_response=False,
            )

            return {
                "type": "info_request",
                "patches": [],
                "explanation": result["completion"],
                "success": True,
                "langchain_used": True,
            }

        except Exception as e:
            logger.error(f"Info request handling failed: {e}")
            return {
                "type": "info_request",
                "error": f"ì •ë³´ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "success": False,
                "langchain_used": True,
            }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
langchain_bedrock_service = LangChainBedrockService()
